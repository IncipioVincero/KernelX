//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-33567101
// Cuda compilation tools, release 12.3, V12.3.107
// Based on NVVM 7.0.1
//

.version 8.3
.target sm_52
.address_size 64

	// .globl	_Z20convolution_with_ptxPfPKfS_iii
// _ZZ20convolution_with_ptxPfPKfS_iiiE4N_ds has been demoted

.visible .entry _Z20convolution_with_ptxPfPKfS_iii(
	.param .u64 _Z20convolution_with_ptxPfPKfS_iii_param_0,
	.param .u64 _Z20convolution_with_ptxPfPKfS_iii_param_1,
	.param .u64 _Z20convolution_with_ptxPfPKfS_iii_param_2,
	.param .u32 _Z20convolution_with_ptxPfPKfS_iii_param_3,
	.param .u32 _Z20convolution_with_ptxPfPKfS_iii_param_4,
	.param .u32 _Z20convolution_with_ptxPfPKfS_iii_param_5
)
{
	.reg .pred 	%p<13>;
	.reg .f32 	%f<81>;
	.reg .b32 	%r<48>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<83>;
	// demoted variable
	.shared .align 4 .b8 _ZZ20convolution_with_ptxPfPKfS_iiiE4N_ds[1600];

	ld.param.u64 	%rd33, [_Z20convolution_with_ptxPfPKfS_iii_param_0];
	ld.param.u64 	%rd34, [_Z20convolution_with_ptxPfPKfS_iii_param_1];
	ld.param.u64 	%rd35, [_Z20convolution_with_ptxPfPKfS_iii_param_2];
	ld.param.u32 	%r4, [_Z20convolution_with_ptxPfPKfS_iii_param_3];
	ld.param.u32 	%r5, [_Z20convolution_with_ptxPfPKfS_iii_param_4];
	ld.param.u32 	%r6, [_Z20convolution_with_ptxPfPKfS_iii_param_5];
	setp.lt.s32 	%p3, %r4, 1;
	@%p3 bra 	$L__BB0_7;

	cvta.to.global.u64 	%rd1, %rd34;
	mov.u32 	%r8, %tid.y;
	shl.b32 	%r9, %r8, 4;
	mov.u32 	%r10, %tid.x;
	add.s32 	%r11, %r9, %r10;
	mul.hi.s32 	%r12, %r11, 1717986919;
	shr.u32 	%r13, %r12, 31;
	shr.s32 	%r14, %r12, 3;
	add.s32 	%r15, %r14, %r13;
	mul.lo.s32 	%r16, %r15, 20;
	sub.s32 	%r17, %r11, %r16;
	mov.u32 	%r18, %ctaid.y;
	shl.b32 	%r19, %r18, 4;
	add.s32 	%r20, %r19, %r15;
	add.s32 	%r21, %r20, -2;
	mov.u32 	%r22, %ctaid.x;
	shl.b32 	%r23, %r22, 4;
	add.s32 	%r24, %r23, %r17;
	add.s32 	%r25, %r24, -2;
	setp.lt.s32 	%p4, %r21, %r6;
	or.b32  	%r26, %r21, %r25;
	setp.gt.s32 	%p5, %r26, -1;
	and.pred  	%p6, %p5, %p4;
	setp.lt.s32 	%p7, %r25, %r5;
	and.pred  	%p1, %p7, %p6;
	mov.u32 	%r27, _ZZ20convolution_with_ptxPfPKfS_iiiE4N_ds;
	mad.lo.s32 	%r28, %r15, 80, %r27;
	shl.b32 	%r29, %r17, 2;
	add.s32 	%r1, %r28, %r29;
	add.s32 	%r30, %r19, %r8;
	add.s32 	%r31, %r23, %r10;
	setp.lt.s32 	%p8, %r30, %r6;
	setp.lt.s32 	%p9, %r31, %r5;
	and.pred  	%p2, %p9, %p8;
	mul.wide.u32 	%rd36, %r8, 80;
	{ .reg .b64 %tmp;
	  cvt.u64.u32 	%tmp, %r27;
	  cvta.shared.u64 	%rd37, %tmp; }
	add.s64 	%rd38, %rd37, %rd36;
	mul.wide.u32 	%rd39, %r10, 4;
	add.s64 	%rd2, %rd38, %rd39;
	add.s32 	%r32, %r10, 1;
	mul.wide.u32 	%rd40, %r32, 4;
	add.s64 	%rd3, %rd38, %rd40;
	add.s32 	%r33, %r10, 2;
	mul.wide.u32 	%rd41, %r33, 4;
	add.s64 	%rd4, %rd38, %rd41;
	add.s32 	%r34, %r10, 3;
	mul.wide.u32 	%rd42, %r34, 4;
	add.s64 	%rd5, %rd38, %rd42;
	add.s32 	%r35, %r10, 4;
	mul.wide.u32 	%rd43, %r35, 4;
	add.s64 	%rd6, %rd38, %rd43;
	add.s32 	%r36, %r8, 1;
	mul.wide.u32 	%rd44, %r36, 80;
	add.s64 	%rd45, %rd37, %rd44;
	add.s64 	%rd7, %rd45, %rd39;
	add.s64 	%rd8, %rd45, %rd40;
	add.s64 	%rd9, %rd45, %rd41;
	add.s64 	%rd10, %rd45, %rd42;
	add.s64 	%rd11, %rd45, %rd43;
	add.s32 	%r37, %r8, 2;
	mul.wide.u32 	%rd46, %r37, 80;
	add.s64 	%rd47, %rd37, %rd46;
	add.s64 	%rd12, %rd47, %rd39;
	add.s64 	%rd13, %rd47, %rd40;
	add.s64 	%rd14, %rd47, %rd41;
	add.s64 	%rd15, %rd47, %rd42;
	add.s64 	%rd16, %rd47, %rd43;
	add.s32 	%r38, %r8, 3;
	mul.wide.u32 	%rd48, %r38, 80;
	add.s64 	%rd49, %rd37, %rd48;
	add.s64 	%rd17, %rd49, %rd39;
	add.s64 	%rd18, %rd49, %rd40;
	add.s64 	%rd19, %rd49, %rd41;
	add.s64 	%rd20, %rd49, %rd42;
	add.s64 	%rd21, %rd49, %rd43;
	add.s32 	%r39, %r8, 4;
	mul.wide.u32 	%rd50, %r39, 80;
	add.s64 	%rd51, %rd37, %rd50;
	add.s64 	%rd22, %rd51, %rd39;
	add.s64 	%rd23, %rd51, %rd40;
	add.s64 	%rd24, %rd51, %rd41;
	add.s64 	%rd25, %rd51, %rd42;
	add.s64 	%rd26, %rd51, %rd43;
	mad.lo.s32 	%r40, %r5, %r21, %r17;
	add.s32 	%r41, %r40, %r23;
	add.s32 	%r42, %r41, -2;
	mul.lo.s32 	%r43, %r4, %r42;
	cvta.to.global.u64 	%rd52, %rd33;
	mul.wide.s32 	%rd53, %r43, 4;
	add.s64 	%rd82, %rd52, %rd53;
	mad.lo.s32 	%r44, %r5, %r30, %r10;
	add.s32 	%r45, %r44, %r23;
	mul.lo.s32 	%r46, %r4, %r45;
	cvta.to.global.u64 	%rd54, %rd35;
	mul.wide.s32 	%rd55, %r46, 4;
	add.s64 	%rd81, %rd54, %rd55;
	mov.u32 	%r47, 0;
	not.pred 	%p10, %p1;
	not.pred 	%p11, %p2;

$L__BB0_2:
	mov.f32 	%f80, 0f00000000;
	@%p10 bra 	$L__BB0_4;

	ld.global.f32 	%f80, [%rd82];

$L__BB0_4:
	st.shared.f32 	[%r1], %f80;
	bar.sync 	0;
	// begin inline asm
	ld.shared.f32 %f5, [%rd2];
	// end inline asm
	ld.global.nc.f32 	%f30, [%rd1];
	fma.rn.f32 	%f31, %f5, %f30, 0f00000000;
	// begin inline asm
	ld.shared.f32 %f6, [%rd3];
	// end inline asm
	ld.global.nc.f32 	%f32, [%rd1+4];
	fma.rn.f32 	%f33, %f6, %f32, %f31;
	// begin inline asm
	ld.shared.f32 %f7, [%rd4];
	// end inline asm
	ld.global.nc.f32 	%f34, [%rd1+8];
	fma.rn.f32 	%f35, %f7, %f34, %f33;
	// begin inline asm
	ld.shared.f32 %f8, [%rd5];
	// end inline asm
	ld.global.nc.f32 	%f36, [%rd1+12];
	fma.rn.f32 	%f37, %f8, %f36, %f35;
	// begin inline asm
	ld.shared.f32 %f9, [%rd6];
	// end inline asm
	ld.global.nc.f32 	%f38, [%rd1+16];
	fma.rn.f32 	%f39, %f9, %f38, %f37;
	// begin inline asm
	ld.shared.f32 %f10, [%rd7];
	// end inline asm
	ld.global.nc.f32 	%f40, [%rd1+20];
	fma.rn.f32 	%f41, %f10, %f40, %f39;
	// begin inline asm
	ld.shared.f32 %f11, [%rd8];
	// end inline asm
	ld.global.nc.f32 	%f42, [%rd1+24];
	fma.rn.f32 	%f43, %f11, %f42, %f41;
	// begin inline asm
	ld.shared.f32 %f12, [%rd9];
	// end inline asm
	ld.global.nc.f32 	%f44, [%rd1+28];
	fma.rn.f32 	%f45, %f12, %f44, %f43;
	// begin inline asm
	ld.shared.f32 %f13, [%rd10];
	// end inline asm
	ld.global.nc.f32 	%f46, [%rd1+32];
	fma.rn.f32 	%f47, %f13, %f46, %f45;
	// begin inline asm
	ld.shared.f32 %f14, [%rd11];
	// end inline asm
	ld.global.nc.f32 	%f48, [%rd1+36];
	fma.rn.f32 	%f49, %f14, %f48, %f47;
	// begin inline asm
	ld.shared.f32 %f15, [%rd12];
	// end inline asm
	ld.global.nc.f32 	%f50, [%rd1+40];
	fma.rn.f32 	%f51, %f15, %f50, %f49;
	// begin inline asm
	ld.shared.f32 %f16, [%rd13];
	// end inline asm
	ld.global.nc.f32 	%f52, [%rd1+44];
	fma.rn.f32 	%f53, %f16, %f52, %f51;
	// begin inline asm
	ld.shared.f32 %f17, [%rd14];
	// end inline asm
	ld.global.nc.f32 	%f54, [%rd1+48];
	fma.rn.f32 	%f55, %f17, %f54, %f53;
	// begin inline asm
	ld.shared.f32 %f18, [%rd15];
	// end inline asm
	ld.global.nc.f32 	%f56, [%rd1+52];
	fma.rn.f32 	%f57, %f18, %f56, %f55;
	// begin inline asm
	ld.shared.f32 %f19, [%rd16];
	// end inline asm
	ld.global.nc.f32 	%f58, [%rd1+56];
	fma.rn.f32 	%f59, %f19, %f58, %f57;
	// begin inline asm
	ld.shared.f32 %f20, [%rd17];
	// end inline asm
	ld.global.nc.f32 	%f60, [%rd1+60];
	fma.rn.f32 	%f61, %f20, %f60, %f59;
	// begin inline asm
	ld.shared.f32 %f21, [%rd18];
	// end inline asm
	ld.global.nc.f32 	%f62, [%rd1+64];
	fma.rn.f32 	%f63, %f21, %f62, %f61;
	// begin inline asm
	ld.shared.f32 %f22, [%rd19];
	// end inline asm
	ld.global.nc.f32 	%f64, [%rd1+68];
	fma.rn.f32 	%f65, %f22, %f64, %f63;
	// begin inline asm
	ld.shared.f32 %f23, [%rd20];
	// end inline asm
	ld.global.nc.f32 	%f66, [%rd1+72];
	fma.rn.f32 	%f67, %f23, %f66, %f65;
	// begin inline asm
	ld.shared.f32 %f24, [%rd21];
	// end inline asm
	ld.global.nc.f32 	%f68, [%rd1+76];
	fma.rn.f32 	%f69, %f24, %f68, %f67;
	// begin inline asm
	ld.shared.f32 %f25, [%rd22];
	// end inline asm
	ld.global.nc.f32 	%f70, [%rd1+80];
	fma.rn.f32 	%f71, %f25, %f70, %f69;
	// begin inline asm
	ld.shared.f32 %f26, [%rd23];
	// end inline asm
	ld.global.nc.f32 	%f72, [%rd1+84];
	fma.rn.f32 	%f73, %f26, %f72, %f71;
	// begin inline asm
	ld.shared.f32 %f27, [%rd24];
	// end inline asm
	ld.global.nc.f32 	%f74, [%rd1+88];
	fma.rn.f32 	%f75, %f27, %f74, %f73;
	// begin inline asm
	ld.shared.f32 %f28, [%rd25];
	// end inline asm
	ld.global.nc.f32 	%f76, [%rd1+92];
	fma.rn.f32 	%f77, %f28, %f76, %f75;
	// begin inline asm
	ld.shared.f32 %f29, [%rd26];
	// end inline asm
	ld.global.nc.f32 	%f78, [%rd1+96];
	fma.rn.f32 	%f3, %f29, %f78, %f77;
	@%p11 bra 	$L__BB0_6;

	cvt.f64.f32 	%fd1, %f3;
	cvt.sat.f64.f64 	%fd2, %fd1;
	cvt.rn.f32.f64 	%f79, %fd2;
	st.global.f32 	[%rd81], %f79;

$L__BB0_6:
	bar.sync 	0;
	add.s64 	%rd82, %rd82, 4;
	add.s64 	%rd81, %rd81, 4;
	add.s32 	%r47, %r47, 1;
	setp.lt.s32 	%p12, %r47, %r4;
	@%p12 bra 	$L__BB0_2;

$L__BB0_7:
	ret;

}

